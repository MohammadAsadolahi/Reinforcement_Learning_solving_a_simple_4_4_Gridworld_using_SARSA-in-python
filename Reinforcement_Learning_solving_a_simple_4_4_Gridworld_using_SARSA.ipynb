{"metadata":{"colab":{"collapsed_sections":[],"name":"Reinforcement-Learning-solving-a-simple-4-4-Gridworld-using-SARSA","toc_visible":true,"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# WRITTEN BY MOHAMMAD ASADOLAHI\n# Mohammad.E.Asadolahi@gmail.com\n# https://github.com/mohammadAsadolahi","metadata":{}},{"cell_type":"markdown","source":"* solving a simple 4*4 Gridworld almost similar to openAI gym FrozenLake using SARSA Temporal difference method Reinforcement Learning\n* this implementation is not a deep learning SARSA implementation. its deployed using Qtable","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport copy","metadata":{"execution":{"iopub.status.busy":"2023-07-13T08:33:18.167192Z","iopub.execute_input":"2023-07-13T08:33:18.167607Z","iopub.status.idle":"2023-07-13T08:33:18.202372Z","shell.execute_reply.started":"2023-07-13T08:33:18.167574Z","shell.execute_reply":"2023-07-13T08:33:18.201255Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"class GridWorld:\n    def __init__(self):\n        # S O O O\n        # O O O *\n        # O * O O\n        # O * 0 T\n        self.actionSpace = ('U', 'D', 'L', 'R')\n        self.actions = {\n            (0, 0): ('D', 'R'),\n            (0, 1): ('L', 'D', 'R'),\n            (0, 2): ('L', 'D', 'R'),\n            (0, 3): ('L', 'D'),\n            (1, 0): ('U', 'D', 'R'),\n            (1, 1): ('U', 'L', 'D', 'R'),\n            (1, 2): ('U', 'L', 'D', 'R'),\n            (1, 3): ('U', 'L', 'D'),\n            (2, 0): ('U', 'D', 'R'),\n            (2, 1): ('U', 'L', 'D', 'R'),\n            (2, 2): ('U', 'L', 'D', 'R'),\n            (2, 3): ('U', 'L', 'D'),\n            (3, 0): ('U', 'R'),\n            (3, 1): ('U', 'L', 'R'),\n            (3, 2): ('U', 'L', 'R')\n            #(3,3) isn't included because its the terminal state\n        }\n        self.rewards = {(3, 3): 0.5, (1, 3): -0.5, (2, 1):-0.5, (3, 1):-0.5}\n        \n    def reset(self):\n        self.state = (0, 0)\n        return self.state\n        \n    def is_terminal(self, s):\n        return s not in self.actions\n\n    def getNewState(self,state,action):\n      i, j = zip(state)\n      row = int(i[0])\n      column = int(j[0])\n      if action == 'U':\n          row -= 1\n      elif action == 'D':\n          row += 1\n      elif action == 'L':\n          column -= 1\n      elif action == 'R':\n          column += 1\n      return row,column\n\n    def move(self, action):\n        row,column=self.getNewState(self.state,action)\n        self.state=(row, column)\n        if (row, column) in self.rewards:\n            return (row, column),self.rewards[(row, column)],self.is_terminal(self.state)\n        return (row, column),-0.01,self.is_terminal(self.state)","metadata":{"execution":{"iopub.status.busy":"2023-07-13T08:53:30.581958Z","iopub.execute_input":"2023-07-13T08:53:30.582403Z","iopub.status.idle":"2023-07-13T08:53:30.599221Z","shell.execute_reply.started":"2023-07-13T08:53:30.582370Z","shell.execute_reply":"2023-07-13T08:53:30.597782Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"class Agent:\n    def __init__(self,action_space, exploreRate=0.01):\n        self.qTable = None\n        self.action_space=action_space\n        self.exploreRate= exploreRate\n        self.initial_random_policy()\n        self.initialQtable()\n        self.explored = 0\n        self.exploited = 0\n\n    def initialQtable(self):\n        self.qTable = {}\n        for state in self.action_space:\n            self.qTable[state]={}\n            for move in self.action_space[state]:\n                self.qTable[state][move]=0\n        print(self.qTable)\n        \n    def updateQtable(self, newQ,updateRate=0.05):\n        for state in self.qTable:\n            for action in self.qTable[state]:\n                self.qTable[state][action] = self.qTable[state][action]+(updateRate*(newQ[state][action]-self.qTable[state][action]))\n    \n    def chooseAction(self, state):\n        if self.exploreRate > np.random.rand():\n            self.explored += 1\n            return np.random.choice(self.action_space[state])\n        self.exploited += 1\n        return self.policy[state]\n    \n    def initial_random_policy(self):\n        self.policy = {}\n        for state in self.action_space:\n            self.policy[state] = np.random.choice(self.action_space[state])\n            \n    def learn(self,state,nextState,reward,done):\n        if not done:\n            targetQ= reward + (0.9 * self.qTable[nextState][self.chooseAction(nextState)])\n            self.qTable[state][action]=self.qTable[state][action]+alpha*(targetQ - self.qTable[state][action])\n    \n    def update_policy(self):\n        for state in self.policy:\n            self.policy[state] = max(self.qTable[state], key=self.qTable[state].get)","metadata":{"execution":{"iopub.status.busy":"2023-07-13T08:53:49.366584Z","iopub.execute_input":"2023-07-13T08:53:49.367013Z","iopub.status.idle":"2023-07-13T08:53:49.382740Z","shell.execute_reply.started":"2023-07-13T08:53:49.366965Z","shell.execute_reply":"2023-07-13T08:53:49.381332Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"def printPolicy(policy):\n        line = \"\"\n        counter = 0\n        for item in policy:\n            line += f\" | {policy[item]} | \"\n            counter += 1\n            if counter > 3:\n                print(line)\n                print(\"----------------------------\")\n                counter = 0\n                line = \"\"\n        print(line)\n        print(\"----------------------------\")","metadata":{"execution":{"iopub.status.busy":"2023-07-13T08:54:49.208805Z","iopub.execute_input":"2023-07-13T08:54:49.209302Z","iopub.status.idle":"2023-07-13T08:54:49.216007Z","shell.execute_reply.started":"2023-07-13T08:54:49.209262Z","shell.execute_reply":"2023-07-13T08:54:49.214813Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"env=GridWorld()\nagent = Agent(env.actions)\n\n# policy = {(0, 0): 'R', (0, 1): 'R', (0, 2): 'D', (0, 3): 'L', (1, 0): 'U', (1, 1): 'R', (1, 2): 'D', (1, 3): 'D'\n#     ,(2, 0): 'D', (2, 1): 'R', (2, 2): 'R', (2, 3): 'D', (3, 0): 'R', (3, 1): 'R', (3, 2): 'R'}\n# env.printPolicy(policy)\n\nalpha=0.1\nfor i in range(2000):\n    state = env.reset()\n    stepCounts=0\n    done=False\n    while not done and (stepCounts<20):\n        action=agent.chooseAction(state)\n        nextState, reward, done = env.move(action)\n        stepCounts += 1\n        targetQ=reward\n        agent.learn(state,nextState, reward, done )\n        state = nextState\n    agent.update_policy()\n    if i%200==0:\n        print(f\"\\n\\n\\n step:{i}\")\n        printPolicy(agent.policy)\n        print(\"\\n\")\nprint(f\"exploited:{agent.exploited}  explored:{agent.explored}\")","metadata":{"id":"VEg6wmxJzET8","colab":{"base_uri":"https://localhost:8080/"},"outputId":"a378d49e-6a53-4a13-db71-344352c85fa3","execution":{"iopub.status.busy":"2023-07-13T08:55:35.982241Z","iopub.execute_input":"2023-07-13T08:55:35.982674Z","iopub.status.idle":"2023-07-13T08:55:36.176979Z","shell.execute_reply.started":"2023-07-13T08:55:35.982640Z","shell.execute_reply":"2023-07-13T08:55:36.175735Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"{(0, 0): {'D': 0, 'R': 0}, (0, 1): {'L': 0, 'D': 0, 'R': 0}, (0, 2): {'L': 0, 'D': 0, 'R': 0}, (0, 3): {'L': 0, 'D': 0}, (1, 0): {'U': 0, 'D': 0, 'R': 0}, (1, 1): {'U': 0, 'L': 0, 'D': 0, 'R': 0}, (1, 2): {'U': 0, 'L': 0, 'D': 0, 'R': 0}, (1, 3): {'U': 0, 'L': 0, 'D': 0}, (2, 0): {'U': 0, 'D': 0, 'R': 0}, (2, 1): {'U': 0, 'L': 0, 'D': 0, 'R': 0}, (2, 2): {'U': 0, 'L': 0, 'D': 0, 'R': 0}, (2, 3): {'U': 0, 'L': 0, 'D': 0}, (3, 0): {'U': 0, 'R': 0}, (3, 1): {'U': 0, 'L': 0, 'R': 0}, (3, 2): {'U': 0, 'L': 0, 'R': 0}}\n\n\n\n step:0\n | R |  | L |  | L |  | L | \n----------------------------\n | U |  | U |  | U |  | U | \n----------------------------\n | U |  | U |  | U |  | U | \n----------------------------\n | R |  | U |  | U | \n----------------------------\n\n\n\n\n\n step:200\n | R |  | R |  | D |  | L | \n----------------------------\n | D |  | U |  | D |  | D | \n----------------------------\n | U |  | L |  | D |  | D | \n----------------------------\n | U |  | L |  | R | \n----------------------------\n\n\n\n\n\n step:400\n | R |  | R |  | D |  | L | \n----------------------------\n | D |  | R |  | D |  | D | \n----------------------------\n | D |  | L |  | R |  | D | \n----------------------------\n | U |  | L |  | R | \n----------------------------\n\n\n\n\n\n step:600\n | R |  | R |  | D |  | L | \n----------------------------\n | U |  | R |  | D |  | D | \n----------------------------\n | U |  | L |  | R |  | D | \n----------------------------\n | U |  | L |  | R | \n----------------------------\n\n\n\n\n\n step:800\n | R |  | R |  | D |  | L | \n----------------------------\n | U |  | R |  | D |  | D | \n----------------------------\n | U |  | L |  | R |  | D | \n----------------------------\n | U |  | L |  | R | \n----------------------------\n\n\n\n\n\n step:1000\n | R |  | R |  | D |  | L | \n----------------------------\n | U |  | R |  | D |  | D | \n----------------------------\n | U |  | L |  | R |  | D | \n----------------------------\n | U |  | L |  | R | \n----------------------------\n\n\n\n\n\n step:1200\n | R |  | D |  | D |  | L | \n----------------------------\n | U |  | R |  | D |  | D | \n----------------------------\n | U |  | L |  | R |  | D | \n----------------------------\n | U |  | L |  | R | \n----------------------------\n\n\n\n\n\n step:1400\n | R |  | D |  | D |  | L | \n----------------------------\n | U |  | R |  | D |  | L | \n----------------------------\n | U |  | L |  | R |  | D | \n----------------------------\n | U |  | L |  | R | \n----------------------------\n\n\n\n\n\n step:1600\n | R |  | D |  | D |  | L | \n----------------------------\n | U |  | R |  | D |  | L | \n----------------------------\n | U |  | L |  | R |  | D | \n----------------------------\n | U |  | L |  | R | \n----------------------------\n\n\n\n\n\n step:1800\n | R |  | R |  | D |  | L | \n----------------------------\n | R |  | R |  | D |  | L | \n----------------------------\n | U |  | L |  | R |  | D | \n----------------------------\n | U |  | L |  | R | \n----------------------------\n\n\nexploited:24995  explored:236\n","output_type":"stream"}]}]}
